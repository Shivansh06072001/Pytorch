{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDENTIFYING HANDWRITTEN DIGITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aim: To compare the performance of a feed-forward neural network with different number of hidden layers\n",
    "###### NOTE: CODE COULD HAVE BEEN A LOT SHORTER , BUT SINCE THIS WAS A SPONTANEOUS EXPERIMENT , IT MAY BE A BIT REPETITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms.ToTensor() - converts the images into numbers understandable by the sysem (R,B,G)\n",
    "#transforms.Noramlize() - normalizes the tensor with a meann and std dev - parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images , labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape\n",
    "#64 - batch size , each image has 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape\n",
    "#64 images have 64 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20324725b50>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(),cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING NEURAL NETWORK  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With one hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size =128\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_size,hidden_size), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_size, output_size),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3110, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "images,labels = next(iter(trainloader))\n",
    "\n",
    "#converting images to (64,784)\n",
    "images  = images.view(images.shape[0],-1)\n",
    "\n",
    "logps = model(images)\n",
    "\n",
    "loss = criterion(logps,labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward pass:  None\n",
      "after backward pass:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#adjusting the weights\n",
    "print(\"before backward pass: \",model[0].weight.grad)\n",
    "loss.backward()\n",
    "print(\"after backward pass: \",model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images,labels in trainloader:\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion1(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.9304\n"
     ]
    }
   ],
   "source": [
    "correct_count , total_count = 0,0\n",
    "for images,labels in testloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1,784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "            \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count+=1;\n",
    "        total_count+=1\n",
    "\n",
    "print('accuracy : ' , correct_count/total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 2 hidden layers (Negative Loss Likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_sizes = [128,64]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_size,hidden_sizes[0]), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(hidden_sizes[0],hidden_sizes[1]), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn sequential wraps the layers in the network\n",
    "\n",
    "#log soft max is used because it is a classification problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function used here is negative log likelihood function\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3126, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "images,labels = next(iter(trainloader))\n",
    "\n",
    "#converting images to (64,784)\n",
    "images  = images.view(images.shape[0],-1)\n",
    "\n",
    "logps = model(images)\n",
    "\n",
    "loss = criterion(logps,labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward pass:  None\n",
      "after backward pass:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#adjusting the weights\n",
    "print(\"before backward pass: \",model[0].weight.grad)\n",
    "loss.backward()\n",
    "print(\"after backward pass: \",model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images,labels in trainloader:\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit =  7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x203253e80d0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "img = images[0].view(1,784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit = \",probab.index(max(probab)))\n",
    "plt.imshow(images[0].numpy().squeeze(),cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.9335\n"
     ]
    }
   ],
   "source": [
    "correct_count , total_count = 0,0\n",
    "for images,labels in testloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1,784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "            \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count+=1;\n",
    "        total_count+=1\n",
    "\n",
    "print('accuracy : ' , correct_count/total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_sizes = [128,64,32]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_size,hidden_size), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0],hidden_sizes[1]), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1],hidden_sizes[2]), \n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3171, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "images,labels = next(iter(trainloader))\n",
    "\n",
    "#converting images to (64,784)\n",
    "images  = images.view(images.shape[0],-1)\n",
    "\n",
    "logps = model(images)\n",
    "\n",
    "loss = criterion(logps,labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward pass:  None\n",
      "after backward pass:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#adjusting the weights\n",
    "print(\"before backward pass: \",model[0].weight.grad)\n",
    "loss.backward()\n",
    "print(\"after backward pass: \",model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images,labels in trainloader:\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion1(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit =  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20325384cd0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "img = images[0].view(1,784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit = \",probab.index(max(probab)))\n",
    "plt.imshow(images[0].numpy().squeeze(),cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.9387\n"
     ]
    }
   ],
   "source": [
    "correct_count , total_count = 0,0\n",
    "for images,labels in testloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1,784)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "            \n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count+=1;\n",
    "        total_count+=1\n",
    "\n",
    "print('accuracy : ' , correct_count/total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of hidden layers increased , the accuracy of the model also increased\n",
    "1 hidden layer - 0.9304 \n",
    "2 hidden layers - 0.9335\n",
    "3 hidden layers - 0.9387\n",
    "\n",
    "However , we can see that the increase in performance is not significant. Thus it is not preffered to have more than 2 hidden layers if we are working on the same dataset in the same way as it will just increase the complexity of the program with no significant addvantage in accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
